\section{Project Summary}
Imagine observing only a subset of the entries of some true underlying matrix and attempting to fill in the missing entries from the observed entries. Let $Y$ be the observed matrix with missing entries, $X$ be the true underlying matrix, and $\hat{X}$ be the ``filled in" version of $Y$. This problem, known as \textbf{matrix completion}, seems impossible without imposing any further restrictions, as the entries could be filled arbitrarily. However, if the underlying matrix is assumed to have some structure (such as low rank), then recovery may be feasible.

The most famous application of matrix completion is its use in collaborative filtering for recommender systems. Collaborative filtering tries to predict the preferences of a single user through aggregating the preferences of many users and assuming that if two people share the same preference for one item, then they are more likely to share preferences on other items. Most famously, Netflix held a competition with a 1 million dollar prize for the best collaborative filtering algorithm to recommend movies to its users. One can think of the data provided by Netflix as a big matrix, with each user as a row and each movie as a column. Because users only view and rate a subset of all possible movies, the data is inherently incomplete. Utilization of matrix completion allows Netflix to fill in reviews for each user, thus allowing for more personalized recommendations.

Consider an alternative case where users simply ``like" or ``dislike" each movie instead of providing numerical ratings. In this case, only one bit of data is observed with each rating, and assumptions of low rank may no longer hold on the ``filled in" version of $Y$. Furthermore, since one bit data does not capture how strongly a user liked or disliked the movie, learning an underlying parameter which governs the probability of a user liking or disliking a movie may be more applicable. This necessitated a recent extension of matrix completion to the case of observing an incomplete matrix of observations $Y$ whose entries are either $+1$ or $-1$. Called \textbf{one-bit matrix completion}, this assumes each is bit is observed as follows: $Y_{ij} = +1$ with probability $f(M_{ij})$ and $Y_{ij} = -1$ with probability $1 - f(M_{ij})$, where $M$ is a latent matrix that parametrizes $f:\mathbb{R} \rightarrow [0,1]$. 1BMC attempts to recover the matrix $M$ from $Y$ and the function $f(\cdot)$ through maximizing the likelihood of the observed values.

This project will survey multiple matrix completion algorithms, focusing specifically on various methods of low-rank matrix completion and one-bit matrix completion. In addition to presenting the mathematical techniques used in matrix completion (Alternating projections, regularization, and majorize-minimize algorithms), this project will implement four different algorithms and compare their performance on synthetic datasets. Effects of parameters such as the percentage of observed entries, the true rank of $X$, and the dimensions of $X$ on the recovery error will be investigated. The four algorithms implemented are as follows:
\begin{itemize}
    \item Alternating projection method for noiseless observations 
    \item Alternating projection method with regularization (exact rank and nucear norm) for noisy observations 
    \item Projected gradient descent with nuclear norm regularization 
    \item One-bit matrix completion for $f(x) = (1 + e^{-x})^{-1}$ (logistic model)
\end{itemize}