\section{Detailed Project Description}
\paragraph{\textbf{Low-rank Matrix Completion (LRMC) Motivation and Background.}} Modern online platforms, such as Netflix, attempt to present the user with highly personalized search results. However, tailoring search results can be difficult, especially when users only rate a small fraction of the movies they watch. Imagine all of the data that Netflix has stored in one giant matrix where each row represents a user, each column represents a movie, and the $(i,j)^{th}$ entry represents user $i$'s rating for movie $j$. Naturally, many entries of this data matrix are missing simply because users will not view every single movie on Netflix's platform. So how can Netflix deliver personalized movie recommendations to users? Collaborative filtering, which tries to predict the preferences of a single user through aggregating the preferences of many users, is a likely solution. Collaborative filtering assumes that if two users share the same preference for one item, they are more likely to share preferences on other items.

 The structure imposed by collaborative filtering translates nicely to a low-rank constraint on the latent data matrix (the data matrix where every entry is filled in according to the user's preference). The idea is that users who provide the same rating for a movie are more likely to provide similar ratings for other movies, which creates linear dependencies in the latent matrix. Therefore, if Netflix had access to this latent data matrix, it would have a better understanding of which films to recommend to different users. \textbf{Low-rank matrix completion (LRMC)} is a field that attempts to solve this very problem. Suppose there exists some true unobserved matrix $X \in \mathbb{R}^{M \times N}$, and an observed matrix $Y \in \mathbb{R}^{M \times N}$ with $Y_{ij} = y_{ij}$ if $(i,j) \in \Omega$ and $Y_{ij} = \text{Missing}$ if $(i,j) \notin \Omega$, where $\Omega$ is the set of indices of observed entries of $Y$. Assuming $X$ is rank $r \ll \text{min}(M, N)$, LRMC attempts to recover $X$ from $Y$ and $\Omega$.

\paragraph{\textbf{Relevant LRMC papers.}} The seminal work of \cite{candes2009exact} provided the first bound on the number of samples ($|\Omega|$) needed to perfectly recover a low-rank matrix with high probability in the noiseless setting. Various extensions of the original algorithm were developed including a convex relaxation of an exact rank minimization \cite{candes2010power} and an alternating minimization scheme \cite{cai2010singular}. The case of observing noisy entries of $X$ is considered in \cite{candes2010matrix}. More recently, Prof. Jeffrey Fessler (University of Michigan) presents LRMC techniques with various regularization that utilize alternating projection methods and majorize-minorize optimization techniques \cite{fessler_2019} in a set of course notes.

\paragraph{\textbf{One-bit Matrix Completion (1BMC) Motivation and Background.}} Now suppose that the ratings the users give are taken to the extreme case where a user simply says if they ``liked" or ``disliked" a given movie. In this case, each rating only provides one bit of information, which may violate the low-rank constraint on a ``filled in" version of $Y$. Furthermore, since one bit data does not capture how strongly a user liked or disliked the movie, learning an underlying parameter which governs the probability of a user liking or disliking a movie may be more applicable. Essentially, Netflix is assuming some probabilistic model of a user liking a movie and estimating the parameters that govern the model. Then, in order to recommend a movie to a user, Netflix will simply find the movies that have high probability of being liked.

 Recently, matrix completion has been extended to the case of observing an incomplete matrix of observations $Y$ whose entries are either $+1$ or $-1$. Called \textbf{one-bit matrix completion (1BMC)}, this assumes each bit is observed as follows: $Y_{ij} = +1$ with probability $f(M_{ij})$ and $Y_{ij} = -1$ with probability $1 - f(M_{ij})$, where $M$ is a latent matrix that parametrizes $f:\mathbb{R} \rightarrow [0,1]$. 1BMC attempts to recover the matrix $M$ from $Y$ and the function $f(\cdot)$ through maximizing the likelihood of the observed values. 

\paragraph{\textbf{Relevant 1BMC papers.}} One-bit matrix completion was first described by \cite{davenport20141}, which provided a convex program and bounded the recovery error. \cite{7086879} considers the case where the underlying matrix $M$ has an exact low rank structure. Similarly, \cite{mm-spars-2013} considers the special case of pairwise comparison matrices, which have rank 2. The work of \cite{cai2013max} considers a max-norm regularized version of the problem under different sampling schemes. 

\paragraph{\textbf{Project Summary.}} The goal of this project is to survey multiple matrix completion algorithms, focusing specifically on various methods of low-rank matrix completion and one-bit matrix completion. To do so, the project will be divided into the following three sections: 1.) Mathematical theory and background, 2.) Algorithm implementation, and 3.) Results of algorithms on synthetically generated data. Detailed descriptions of each of the sections can be found below.

\paragraph{\textbf{Mathematical theory and background.}} This section will provide all mathematical theory and background information that is not contained in the ECE 6254. Linear algebra building concepts (projections, proximal operators) and advanced optimization techniques (regularization, majorize-minimize algorithms, projected gradient descent, projection onto convex sets) will be built up assuming no information beyond that of ECE 6254. Furthermore, each implemented algorithm will be presented and explained using the discussed concepts. 

\paragraph{\textbf{Algorithm implementation.}} Four algorithms will be implemented from a subset of the cited sources in the previous sections. They are as follows: 
\begin{enumerate}[(1)]
    \item Alternating projection method for noiseless observations (from \cite{fessler_2019})
    \item Alternating projection method with regularization (exact rank and nucear norm) for noisy observations (from \cite{fessler_2019})
    \item Projected gradient descent with nuclear norm regularization (from \cite{cai2010singular})
    \item One-bit matrix completion for $f(x) = (1 + e^{-x})^{-1}$ (from \cite{davenport20141})
\end{enumerate}
These algorithms represent a mix of different variants of the matrix completion problem that allow for a broad survey of mathematical and optimization techniques. The first algorithm utilizes projection onto convex sets algorithm. The second algorithm investigates alternating projections again, but also investigates the effects of regularization and the usage of proximal operators. The third algorithm introduces the concept of projected gradient descent in addition to projections. The fourth algorithm utilizes techniques discussed in ECE 6254 (Maximum likelihood estimation) applied in a novel way. All four of these algorithms will be implemented in MATLAB.

\paragraph{\textbf{Testing on synthetic datasets.}} This section will evaluate the results of the implemented algorithms on synthetic datasets. In the case of LRMC, the effects of various parameters such as the true rank of the underlying matrix ($r$), the dimension of the matrices ($M, N$), the number of sampled indices ($|\Omega|$) on the recovery error will be investigated. Recovery error, for the output of the LRMC algorithms $\hat{X}$, is defined as $\text{err}(\hat{X}, X) = \frac{\norm{X - \hat{X}}_F^2}{\norm{X}_F^2}$, where $\norm{X}_F = \sqrt{\sum\limits_{i = 1}^M\sum\limits_{j = 1}^N |X_{ij}|^2}$ is the matrix Frobenius norm. This error metric looks at the squared error of the difference between the estimated matrix $\hat{X}$ and the true latent matrix $X$ and normalizes it by the Frobenius norm of $X$. The average error and standard deviation across a fixed number of trials will be reported as each parameter is swept. In the case of 1BMC, the same error metric as above will be used to measure the difference between the estimated matrix $\hat{M}$ and the true latent parameter matrix $M$ as the parameters of rank of $M$, dimension of $M$, and number of sampled indices is varied.
